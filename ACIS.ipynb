{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "# ACISDetector\n",
    "<a id=ACISDetector></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture and instruction set\n",
    "<a id=arch_is></a>\n",
    "<!--isadetect \n",
    "\n",
    "@inproceedings{kairajarvi2020isadetect,\n",
    "author={Kairaj\\\"arvi, Sami and Costin, Andrei and H\\\"am\\\"al\\\"ainen, Timo},\n",
    "title={{ISAdetect: Usable Automated Detection of CPU Architecture and Endianness for Executable Binary Files and Object Code}},\n",
    "booktitle={Proceedings of the Tenth ACM Conference on Data and Application Security and Privacy},\n",
    "year={2020},\n",
    "url=\"https://doi.org/10.1145/3374664.3375742\"\n",
    "}\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import isadetect.helpers as isa_api \n",
    "import src.arch_classifier as arch_api\n",
    "import src.hyperparams as hp\n",
    "from src.binary_dataset import FeatureDataset\n",
    "import src.arch_trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/features.csv\n"
     ]
    }
   ],
   "source": [
    "DOWNLOAD_URL = 'https://github.com/cedricdeboom/character-level-rnn-datasets/raw/master/datasets/shakespeare.txt'\n",
    "DATA_DIR = pathlib.Path.home().joinpath('.pytorch-datasets')\n",
    "\n",
    "def download_dataset(out_path=DATA_DIR, url=DOWNLOAD_URL, force=False):\n",
    "    pathlib.Path(out_path).mkdir(exist_ok=True)\n",
    "    out_filename = os.path.join(out_path, os.path.basename(url))\n",
    "    \n",
    "    if os.path.isfile(out_filename) and not force:\n",
    "        print(f'Dataset file {out_filename} exists, skipping download.')\n",
    "    else:\n",
    "        print(f'Downloading {url}...')\n",
    "        with urllib.request.urlopen(url) as response, open(out_filename, 'wb') as out_file:\n",
    "            shutil.copyfileobj(response, out_file)\n",
    "        print(f'Saved to {out_filename}.')\n",
    "    return out_filename\n",
    "\n",
    "#DATASET_FILE = download_dataset()\n",
    "DATASET_FILE = \"./dataset/features.csv\" \n",
    "print(DATASET_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features length: 581\n",
      "Train: 406 samples\n",
      "Test: 175 samples\n",
      "input size = torch.Size([293]) X 1\n"
     ]
    }
   ],
   "source": [
    "binary_dataset = FeatureDataset(DATASET_FILE)\n",
    "N = len(binary_dataset)\n",
    "batch_size = 10\n",
    "print(f'features length: {N}')\n",
    "\n",
    "train_length = int(0.7* N)\n",
    "test_length = N - train_length\n",
    "ds_train,ds_test = torch.utils.data.random_split(binary_dataset,(train_length,test_length))\n",
    "\n",
    "print(f'Train: {len(ds_train)} samples')\n",
    "print(f'Test: {len(ds_test)} samples')\n",
    "\n",
    "dl_train=torch.utils.data.DataLoader(ds_train,batch_size=batch_size, shuffle=True)\n",
    "\n",
    "x0,y0 = ds_train[0]\n",
    "dataset_shape = (x0.shape if x0.dim() > 0 else 1),(y0.shape if y0.dim() > 0 else 1)\n",
    "print('input size =', dataset_shape[0], \"X\",dataset_shape[1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "<a id=part2_3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in order to plot our result and to compare them, we will use plot.py\n",
    "and then we'll use the following function to load multiple experiment results and plot them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function plot_fit in module jupyter_utils.plot:\n",
      "\n",
      "plot_fit(fit_res: jupyter_utils.train_results.FitResult, fig=None, log_loss=False, legend=None)\n",
      "    Plots a FitResult object.\n",
      "    Creates four plots: train loss, test loss, train acc, test acc.\n",
      "    :param fit_res: The fit result to plot.\n",
      "    :param fig: A figure previously returned from this function. If not None,\n",
      "        plots will the added to this figure.\n",
      "    :param log_loss: Whether to plot the losses in log scale.\n",
      "    :param legend: What to call this FitResult in the legend.\n",
      "    :return: The figure.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from jupyter_utils.plot import plot_fit\n",
    "fig = None\n",
    "fit_res = []\n",
    "\n",
    "help(plot_fit)\n",
    "\n",
    "def plot_exp_results(filename_pattern, results_dir='results'):\n",
    "    fig = None\n",
    "    result_files = glob.glob(os.path.join(results_dir, filename_pattern))\n",
    "    result_files.sort()\n",
    "    if len(result_files) == 0:\n",
    "        print(f'No results found for pattern {filename_pattern}.', file=sys.stderr)\n",
    "        return\n",
    "    for filepath in result_files:\n",
    "        m = re.match('exp\\d_(\\d_)?(.*)\\.json', os.path.basename(filepath))\n",
    "        cfg, fit_res = load_experiment(filepath)\n",
    "        fig, axes = plot_fit(fit_res, fig, legend=m[2],log_loss=True)\n",
    "    del cfg['filters_per_layer']\n",
    "    del cfg['layers_per_block']\n",
    "    print('common config: ', cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wstd': 0.1, 'lr': 0.01, 'reg': 0.05}\n",
      "RandomForest()\n"
     ]
    }
   ],
   "source": [
    "rf_hp = hp.random_forest_hp()\n",
    "print(rf_hp)\n",
    "\n",
    "_randomForest = arch_api.RandomForest(in_estimators = 100 , in_max_depth = 32,random_state= 0 , n_jobs=-1)\n",
    "print(_randomForest)\n",
    "\n",
    "#fit_res.insert(trainer(_randomForest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wstd': 0.1, 'lr_vanilla': 0.01, 'lr_momentum': 0.02, 'lr_rmsprop': 1e-05, 'reg': 0.01}\n"
     ]
    }
   ],
   "source": [
    "mlp_hp = hp.mlp_hp()\n",
    "print(mlp_hp)\n",
    "\n",
    "#_mlp = arch_api.MLP(100,32,0,1,0)\n",
    "#print(_mlp)\n",
    "\n",
    "#fit_res.insert(trainer(_mlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wstd': 0.1, 'lr': 0.001}\n"
     ]
    }
   ],
   "source": [
    "cnn_hp = hp.cnn_hp()\n",
    "print(cnn_hp)\n",
    "\n",
    "#_cnn = arch_api.CNN()\n",
    "#print(_cnn)\n",
    "\n",
    "#fit_res.insert(trainer(_cnn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in fit_res:\n",
    "    fig, axes = plot_fit(fit_res, fig, legend=m[2],log_loss=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
